{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-5987ad60c7b8>, line 151)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-5987ad60c7b8>\"\u001b[1;36m, line \u001b[1;32m151\u001b[0m\n\u001b[1;33m    sess=tf.Session()\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.image.cifar10 import cifar10_input\n",
    "import numpy as np\n",
    "from data_utils import *\n",
    "from batch_norm import *\n",
    "\n",
    "BATCH_SIZE=256\n",
    "DATA_DIR='/tmp/cifar10_data'\n",
    "IMAGE_SIZE = cifar10_input.IMAGE_SIZE\n",
    "NUM_CLASSES = cifar10_input.NUM_CLASSES\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "\n",
    "TOWER_NAME = 'tower'\n",
    "def _activation_summary(x):\n",
    "    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "    # session. This helps the clarity of presentation on tensorboard.\n",
    "    tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "    tf.histogram_summary(tensor_name + '/activations', x)\n",
    "    tf.scalar_summary(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    var = _variable_on_cpu(name, shape,\n",
    "                         tf.truncated_normal_initializer(stddev=stddev))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "\n",
    "\n",
    "# %% Setup input to the network and true output label.  These are\n",
    "# simply placeholders which we'll fill in later.\n",
    "x = tf.placeholder(tf.float32, [None, 32,32,3])\n",
    "y = tf.placeholder(tf.float32, [None, 20])\n",
    "\n",
    "# %% We add a new type of placeholder to denote when we are training.\n",
    "# This will be used to change the way we compute the network during\n",
    "# training/testing.\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "# %% We'll convert our MNIST vector data to a 4-D tensor:\n",
    "# N x W x H x C\n",
    "initfact=10\n",
    "lr=.1\n",
    "path='dataset'\n",
    "\n",
    "\n",
    "kernel = _variable_with_weight_decay('conv0',\n",
    "                                 shape=[3, 3, 3, 16],\n",
    "                                 stddev=np.sqrt(2.0/initfact/3)\n",
    "                                 , wd=0.0)\n",
    "net = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "#net = BatchNorm(net)\n",
    "net = batch_norm(net,is_training,scope='conv0')\n",
    "net = tf.nn.elu(net)\n",
    "#_activation_summary(net)\n",
    "    \n",
    "\n",
    "for block in range(3):\n",
    "    nfilters=16<<block\n",
    "    for layer in range(1):\n",
    "        net_copy=net\n",
    "        for i in range(2):\n",
    "            name = 'block_%d/layer_%d/conv%d' % (block, layer,i)\n",
    "            if block==0:\n",
    "                i=1\n",
    "            kernel = _variable_with_weight_decay(name,\n",
    "                    shape=[3, 3,\n",
    "                           net.get_shape().as_list()[3],\n",
    "                           nfilters],\n",
    "                    stddev=np.sqrt(2.0/initfact/nfilters),\n",
    "                    wd=0.0)\n",
    "            if layer==0 and block!=0 and i==0:\n",
    "                up=1\n",
    "            else:\n",
    "                up=0\n",
    "            net  = tf.nn.conv2d(net,\n",
    "                    kernel,\n",
    "                    [1,1+up,1+up, 1],\n",
    "                    padding='SAME')\n",
    "            net = batch_norm(net,scope=name)\n",
    "            #net = BatchNorm(net)\n",
    "            net = tf.nn.elu(net)\n",
    "            _activation_summary(net)\n",
    "\n",
    "\n",
    "# residual function (identity shortcut)\n",
    "        if net_copy.get_shape().as_list()[1]!=net.get_shape().as_list()[1]:\n",
    "            net_copy=tf.nn.avg_pool(net_copy,[1,2,2,1],\n",
    "                    strides=[1,2,2,1],padding='VALID')\n",
    "            net_copy=tf.pad(net_copy,[[0,0],[0,0],[0,0],[0,int(nfilters/2)]])\n",
    "        net = net + net_copy\n",
    "\n",
    "#Global avg pooling\n",
    "net_shape = net.get_shape().as_list()\n",
    "net = tf.nn.avg_pool(net,\n",
    "              ksize=[1, net_shape[1], net_shape[2], 1],\n",
    "              strides=[1, 1, 1, 1], \n",
    "              padding='VALID',name='global_pooling')\n",
    "net_shape = net.get_shape().as_list()\n",
    "net = tf.reshape(net,\n",
    "        [-1, net_shape[1] * net_shape[2] * net_shape[3]])\n",
    "\n",
    "weights = _variable_with_weight_decay('softmax_w',\n",
    "    [64, NUM_CLASSES],\n",
    "    stddev=1/64.0,\n",
    "    wd=0.0)\n",
    "biases = _variable_on_cpu('softmax_b',\n",
    "    [NUM_CLASSES],\n",
    "    tf.constant_initializer(0.0))\n",
    "softmax_linear = tf.add(tf.matmul(net, weights), biases, name='softmax')\n",
    "y = tf.cast(y, tf.int64)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      softmax_linear, y, name='cross_entropy_per_example')\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    " \n",
    "train_step=tf.train.MomentumOptimizer(lr,.9).minimize(cross_entropy_mean)\n",
    "#_activation_summary(softmax_linear)\n",
    "\n",
    "correct_prediction=tf.equal(tf.argmax(softmax_linear,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,'float')\n",
    "\n",
    "# %% We now create a new session to actually perform the initialization the\n",
    "# variables:\n",
    "sess=tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# %% We'll train in minibatches and report accuracy:\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "# load data\n",
    "Xtr, Ytr, Xte, Yte=loadCIFAR100(path)\n",
    "numTrain=len(Xtr)\n",
    "for epoch_i in range(n_epochs):\n",
    "    for batch_i in range(mnist.train.num_examples // batch_size):\n",
    "        idx=np.random.choice(numTrain,batch_size)\n",
    "        batch_xs=Xtr(idx)\n",
    "        batch_ys=Ytr(idx)\n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: batch_xs, y: batch_ys, is_training: True})\n",
    "    print(sess.run(accuracy,\n",
    "                   feed_dict={\n",
    "                       x: Xte,\n",
    "                       y: Yte,\n",
    "                       is_training: False\n",
    "                   }))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
